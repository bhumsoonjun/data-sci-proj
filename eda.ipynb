{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data presented here are news articles collected from various news sources. Articles from each news source is grouped together are stored inside the same file. Each line of the file are each separated news article with corresponding timestamp. We are only interested in the raw news data part which is mostly text. Here's the link to the file https://archive.ics.uci.edu/dataset/438/health+news+in+twitter.\n",
    "\n",
    "This is a real dataset which will be used to test the performance and compared to the automatically generated ones. The auto gen data are generated during runtime, so it doesn't really make sense to visualize it since it will change from a run to another anyway. Therefore, this will be the only non changing dataset that we will use to test the algorithm.\n",
    "\n",
    "Note that although the data is automatically generated, we can still control the parameters which are the distribution of the sampling, standard deviation, number of clusters, sparsity and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import functools\n",
    "from dataclasses import *\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Util class for computing statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(repr=True)\n",
    "class data_stats:\n",
    "\n",
    "    stds_sum: float\n",
    "    stds_mean: float\n",
    "    stds_median: float\n",
    "    std_max: float\n",
    "    std_min: float\n",
    "    dist_from_origin: float\n",
    "    shape: Any\n",
    "    sparsity: float\n",
    "\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        self.stds = data.std(axis=1)\n",
    "        self.stds_sum = self.stds.sum()\n",
    "        self.stds_mean = self.stds.mean()\n",
    "        self.stds_median = np.median(self.stds)\n",
    "        self.std_max = np.max(self.stds)\n",
    "        self.std_min = np.min(self.stds)\n",
    "        self.dist_from_origin = np.linalg.norm(data.mean(axis=0))\n",
    "        self.shape = data.shape\n",
    "        self.sparsity = 1.0 - np.count_nonzero(data) / float(data.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load data from path. There are multiple clusters (16 in total), we will only select 5 clusters because the array gets very large. Also when there are multiple clusters piling up on each other, it gets really hard to visualize. In any case, in the real implementation, I have made it so that we can dynamically load each cluster by passing in the number of cluster we want to load. Hence, we can still test any values in the range from 2-16.\n",
    "\n",
    "Once loaded, we will combine all the series into one single series for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0       GP workload harming care - BMA poll http://bbc...\n 1       Short people's 'heart risk greater' http://bbc...\n 2       New approach against HIV 'promising' http://bb...\n 3       Coalition 'undermined NHS' - doctors http://bb...\n 4       Review of case against NHS manager http://bbc....\n                               ...                        \n 1994    Researchers use video games to study how sleep...\n 1995    Are energy drinks really that bad for you? htt...\n 1996    Men suffering from #depression may also suffer...\n 1997    #Thanksgiving science: Why #gratitude is good ...\n 1998    Clinton KellyÂ’s fresh and #fruity take on #hol...\n Length: 16936, dtype: object,\n ['bbchealth', 'cbchealth', 'cnnhealth', 'everydayhealth', 'foxnewshealth'],\n array([   0, 3928, 3727, 4044, 3238, 1999]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/health+news+in+twitter/Health-Tweets'\n",
    "\n",
    "dfs = []\n",
    "lens = []\n",
    "names = []\n",
    "k_clus = 5\n",
    "cnt = 0\n",
    "for filename in os.listdir(path):\n",
    "    f = os.path.join(path, filename)\n",
    "    if os.path.isfile(f) and cnt < k_clus:\n",
    "        dfs.append(pd.read_csv(f, sep=\"|\", on_bad_lines='skip', encoding=\"latin1\").iloc[:, -1])\n",
    "        lens.append(dfs[-1].size)\n",
    "        names.append(filename[:-4])\n",
    "        cnt += 1\n",
    "\n",
    "lens = np.array([0] + lens)\n",
    "aggregated_df = pd.concat(dfs)\n",
    "aggregated_df, names, lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since our raw data is a text, before we need to preprocess it into numerical values so that we can use ml algorithm on it. Here we use the TfidVectorizer module to transform words into numerical data. Basically it counts the frequency of each words. We then produce the correponding labels of each data points to pair with the numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]),\n array(['bbchealth', 'bbchealth', 'bbchealth', ..., 'foxnewshealth',\n        'foxnewshealth', 'foxnewshealth'], dtype='<U14'),\n (16936, 30112))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "X = vectorizer.fit_transform(aggregated_df).toarray()\n",
    "y = np.array(functools.reduce(lambda a, b: a + b, [[names[i]] * lens[i + 1] for i in range(len(names))]))\n",
    "X, y, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will use PCA to visualize the data. See that the shape of X is (16936, 30112), and it will be impossible to plot such data. To maximize variance, the best we could do while visualizing data is to plot it as a 3D scatter plot. Therefore, we will choose ethe number of components to be three here. Then, we will apply the dimensionality reduction on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.12508905,  0.01893124, -0.03730875],\n       [ 0.13665667,  0.01262441, -0.05037594],\n       [ 0.13682485,  0.01893929, -0.03562506],\n       ...,\n       [-0.00252938, -0.07758758, -0.03194731],\n       [-0.07043023, -0.07821037,  0.00260349],\n       [-0.02956656, -0.06640642, -0.00736219]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "reduc_X = pca.fit_transform(X)\n",
    "reduc_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we simply calculate the prefix sum fo the length array to use it for indexing for plotting down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([    0,  3928,  7655, 11699, 14937, 16936])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_means = []\n",
    "clusters = []\n",
    "prefix = lens.cumsum()\n",
    "prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we split each cluster here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "([array([0.00010171, 0.00187191, 0.        , ..., 0.        , 0.        ,\n         0.        ]),\n  array([0.        , 0.00224272, 0.        , ..., 0.        , 0.        ,\n         0.        ]),\n  array([0.00000000e+00, 1.33511908e-03, 8.77821625e-05, ...,\n         5.28469375e-05, 8.44114572e-05, 0.00000000e+00]),\n  array([1.78937949e-04, 2.70675201e-04, 0.00000000e+00, ...,\n         0.00000000e+00, 0.00000000e+00, 8.45450557e-05]),\n  array([0.        , 0.00182102, 0.        , ..., 0.        , 0.        ,\n         0.        ])],\n [array([[ 0.12508905,  0.01893124, -0.03730875],\n         [ 0.13665667,  0.01262441, -0.05037594],\n         [ 0.13682485,  0.01893929, -0.03562506],\n         ...,\n         [ 0.1319689 ,  0.01252517, -0.04120252],\n         [ 0.17345353,  0.02422875, -0.05242519],\n         [ 0.13224365,  0.02043836, -0.04432558]]),\n  array([[ 0.07465068,  0.01927082,  0.18906008],\n         [ 0.00328963,  0.0070751 ,  0.16448705],\n         [ 0.0053711 ,  0.00297966,  0.16362168],\n         ...,\n         [-0.02676127, -0.10281136, -0.01088272],\n         [-0.06720518, -0.119844  , -0.03319843],\n         [-0.04376303, -0.11179227,  0.05855027]]),\n  array([[-0.1140869 ,  0.10851768, -0.0308844 ],\n         [-0.1369515 ,  0.12540103, -0.05767338],\n         [-0.07913525,  0.08561588, -0.0185716 ],\n         ...,\n         [-0.08655971,  0.14477833, -0.0190454 ],\n         [-0.13683092,  0.14977272, -0.06307598],\n         [-0.07958746,  0.17970883, -0.03066617]]),\n  array([[-0.10693819, -0.12866931, -0.06493902],\n         [-0.11898778, -0.07720263, -0.08455164],\n         [ 0.02420068, -0.06329963, -0.0651062 ],\n         ...,\n         [-0.0340985 , -0.12369254, -0.0276752 ],\n         [-0.06097497, -0.07827365, -0.03464196],\n         [-0.03571635, -0.06843542, -0.0484124 ]]),\n  array([[-0.03563536, -0.10413049, -0.00760478],\n         [-0.02152563, -0.08123455,  0.00079359],\n         [ 0.01584196, -0.07108005, -0.0155855 ],\n         ...,\n         [-0.00252938, -0.07758758, -0.03194731],\n         [-0.07043023, -0.07821037,  0.00260349],\n         [-0.02956656, -0.06640642, -0.00736219]])],\n 5)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(lens.size - 1):\n",
    "    clusters.append(reduc_X[prefix[i]:prefix[i+1],:])\n",
    "    clusters_means.append(X[prefix[i]:prefix[i+1],:].mean(axis=0))\n",
    "\n",
    "clusters_means, clusters, len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We plot the projected data down by using the 3d scatter plot and label each clusters according to the news source here. Notice that some clusters like the bbc news are very easily separable from each other while some like everydayhealth and foxnews are harder to separate in 3 dimensions. However, it might be possible to find some projection in higher dimensional space such that it will separate each clusters from each other.\n",
    "\n",
    "![pca](pics/pca_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' \\nfig = plt.figure()\\nax = fig.add_subplot(projection=\\'3d\\')\\ncolors = [\"red\", \"blue\", \"green\", \"purple\", \"orange\"]\\nfor i in range(len(clusters)):\\n    temp = clusters[i]\\n    ax.scatter(temp[:, 0], temp[:, 1], temp[:, 2], c=colors[i], label=names[i])\\n\\nax.legend(loc=\"upper left\")\\n'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" UNCOMMENT TO RUN \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "colors = [\"red\", \"blue\", \"green\", \"purple\", \"orange\"]\n",
    "for i in range(len(clusters)):\n",
    "    temp = clusters[i]\n",
    "    ax.scatter(temp[:, 0], temp[:, 1], temp[:, 2], c=colors[i], label=names[i])\n",
    "\n",
    "ax.legend(loc=\"upper left\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can use tsne to visualize the data similarly here. However, since TSNE are very computationally expensive for data with higher number of features, we will use pca to project the data into the smaller subspace and only then perform tsne on the projected data to embed the projected data points to a 3D space for visualization again. Here we chose perplexity to be 40 because the dataset is large, simiarly for early_exacggeration. The intermidiate space that we project the data point to using the pca will have 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhum1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhum1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[  7.1982684 , -28.995901  ,  14.533412  ],\n       [-20.763655  , -16.066372  ,   7.5479674 ],\n       [  1.7262423 , -20.999937  ,   1.3761098 ],\n       ...,\n       [-15.520175  ,   1.3319118 ,  12.025691  ],\n       [-23.136036  ,  -3.1372528 ,  -4.219664  ],\n       [ -0.12938255,   3.1262257 ,  22.0474    ]], dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" UNCOMMENT TO RUN \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=3, perplexity=40, early_exaggeration=50)\n",
    "pca = PCA(n_components=100)\n",
    "first_step_reduction = pca.fit_transform(X)\n",
    "reduc_X = tsne.fit_transform(first_step_reduction)\n",
    "reduc_X\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The clusters here are pretty clump which is somewhat expected because the input matrix X is extremely sparse and the dimension is very high. So using PCA to visualize data is a better choice here.\n",
    "\n",
    "![title](pics/tsne_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nclusters_means = []\\nclusters = []\\nprefix = lens.cumsum()\\nfor i in range(lens.size - 1):\\n    clusters.append(reduc_X[prefix[i]:prefix[i+1],:])\\n    clusters_means.append(X[prefix[i]:prefix[i+1],:].mean(axis=0))\\n\\nclusters_means, clusters, len(clusters)\\n\\nfig = plt.figure()\\nax = fig.add_subplot(projection=\\'3d\\')\\ncolors = [\"red\", \"blue\", \"green\", \"purple\", \"orange\"]\\nfor i in range(len(clusters)):\\n    temp = clusters[i]\\n    ax.scatter(temp[:, 0], temp[:, 1], temp[:, 2], c=colors[i], label=names[i])\\n\\nax.legend(loc=\"upper left\")\\n'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"UNCOMMENT TO RUN\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "clusters_means = []\n",
    "clusters = []\n",
    "prefix = lens.cumsum()\n",
    "for i in range(lens.size - 1):\n",
    "    clusters.append(reduc_X[prefix[i]:prefix[i+1],:])\n",
    "    clusters_means.append(X[prefix[i]:prefix[i+1],:].mean(axis=0))\n",
    "\n",
    "clusters_means, clusters, len(clusters)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "colors = [\"red\", \"blue\", \"green\", \"purple\", \"orange\"]\n",
    "for i in range(len(clusters)):\n",
    "    temp = clusters[i]\n",
    "    ax.scatter(temp[:, 0], temp[:, 1], temp[:, 2], c=colors[i], label=names[i])\n",
    "\n",
    "ax.legend(loc=\"upper left\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we compute further statistic of data. Stds are computed for each basis. Its sum and mean of the stds of each feature are given below. As we can see, the mean of the standard deviation is very low ~ 0.00576, which implies that the data are very closely packed around the center becaus the min and the max are also around the same value.\n",
    "\n",
    "Although the matrix is very large, it is mostly zeroes as the sparsity is very high here (nearly 1) which implies that 99 percent of the entries are zeroes.\n",
    "In conclusion, the data has very high dimension however it is very sparse (99 percent zeroes), and the data points are tightly packed around the origin (0, 0, ..., 0). However, from the pca plot, we know that the data is separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "data_stats(stds_sum=97.57934983373114, stds_mean=0.005761652682671891, stds_median=0.005761757307156004, std_max=0.005762574635993726, std_min=0.005756185691794238, dist_from_origin=0.12231959772827586, shape=(16936, 30112), sparsity=0.9995195703321675)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = data_stats(X)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}