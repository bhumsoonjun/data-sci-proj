<html>
<head>
<title>eda.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
eda.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% md 
</span><span class="s1">The data presented here are news articles collected from various news sources. Articles from each news source is grouped together are stored inside the same file. Each line of the file are each separated news article with corresponding timestamp. We are only interested in the raw news data part which is mostly text. Here's the link to the file https://archive.ics.uci.edu/dataset/438/health+news+in+twitter. 
 
This is a real dataset which will be used to test the performance and compared to the automatically generated ones. The auto gen data are generated during runtime, so it doesn't really make sense to visualize it since it will change from a run to another anyway. Therefore, this will be the only non changing dataset that we will use to test the algorithm. 
 
Note that although the data is automatically generated, we can still control the parameters which are the distribution of the sampling, standard deviation, number of clusters, sparsity and so on. 
</span><span class="s0">#%% 
</span><span class="s1">%matplotlib notebook</span>
<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">from </span><span class="s1">sklearn.decomposition </span><span class="s2">import </span><span class="s1">PCA</span>
<span class="s2">from </span><span class="s1">matplotlib </span><span class="s2">import </span><span class="s1">pyplot </span><span class="s2">as </span><span class="s1">plt</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">sklearn.feature_extraction.text </span><span class="s2">import </span><span class="s1">TfidfVectorizer</span>
<span class="s2">import </span><span class="s1">os</span>
<span class="s2">import </span><span class="s1">functools</span>
<span class="s2">from </span><span class="s1">dataclasses </span><span class="s2">import </span><span class="s1">*</span>
<span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">*</span>
<span class="s0">#%% md 
</span><span class="s1">Util class for computing statistics. 
</span><span class="s0">#%% 
</span><span class="s1">@dataclass(repr=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s2">class </span><span class="s1">data_stats:</span>

    <span class="s1">stds_sum: float</span>
    <span class="s1">stds_mean: float</span>
    <span class="s1">stds_median: float</span>
    <span class="s1">std_max: float</span>
    <span class="s1">std_min: float</span>
    <span class="s1">dist_from_origin: float</span>
    <span class="s1">shape: Any</span>
    <span class="s1">sparsity: float</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">data: np.ndarray):</span>
        <span class="s1">self.stds = data.std(axis=</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">self.stds_sum = self.stds.sum()</span>
        <span class="s1">self.stds_mean = self.stds.mean()</span>
        <span class="s1">self.stds_median = np.median(self.stds)</span>
        <span class="s1">self.std_max = np.max(self.stds)</span>
        <span class="s1">self.std_min = np.min(self.stds)</span>
        <span class="s1">self.dist_from_origin = np.linalg.norm(data.mean(axis=</span><span class="s3">0</span><span class="s1">))</span>
        <span class="s1">self.shape = data.shape</span>
        <span class="s1">self.sparsity = </span><span class="s3">1.0 </span><span class="s1">- np.count_nonzero(data) / float(data.size)</span>

<span class="s0">#%% md 
</span><span class="s1">Load data from path. There are multiple clusters (16 in total), we will only select 5 clusters because the array gets very large. Also when there are multiple clusters piling up on each other, it gets really hard to visualize. In any case, in the real implementation, I have made it so that we can dynamically load each cluster by passing in the number of cluster we want to load. Hence, we can still test any values in the range from 2-16. 
 
Once loaded, we will combine all the series into one single series for further processing. 
</span><span class="s0">#%% 
</span><span class="s1">path = </span><span class="s4">'data/health+news+in+twitter/Health-Tweets'</span>

<span class="s1">dfs = []</span>
<span class="s1">lens = []</span>
<span class="s1">names = []</span>
<span class="s1">k_clus = </span><span class="s3">5</span>
<span class="s1">cnt = </span><span class="s3">0</span>
<span class="s2">for </span><span class="s1">filename </span><span class="s2">in </span><span class="s1">os.listdir(path):</span>
    <span class="s1">f = os.path.join(path</span><span class="s2">, </span><span class="s1">filename)</span>
    <span class="s2">if </span><span class="s1">os.path.isfile(f) </span><span class="s2">and </span><span class="s1">cnt &lt; k_clus:</span>
        <span class="s1">dfs.append(pd.read_csv(f</span><span class="s2">, </span><span class="s1">sep=</span><span class="s4">&quot;|&quot;</span><span class="s2">, </span><span class="s1">on_bad_lines=</span><span class="s4">'skip'</span><span class="s2">, </span><span class="s1">encoding=</span><span class="s4">&quot;latin1&quot;</span><span class="s1">).iloc[:</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s1">lens.append(dfs[-</span><span class="s3">1</span><span class="s1">].size)</span>
        <span class="s1">names.append(filename[:-</span><span class="s3">4</span><span class="s1">])</span>
        <span class="s1">cnt += </span><span class="s3">1</span>

<span class="s1">lens = np.array([</span><span class="s3">0</span><span class="s1">] + lens)</span>
<span class="s1">aggregated_df = pd.concat(dfs)</span>
<span class="s1">aggregated_df</span><span class="s2">, </span><span class="s1">names</span><span class="s2">, </span><span class="s1">lens</span>
<span class="s0">#%% md 
</span><span class="s1">Since our raw data is a text, before we need to preprocess it into numerical values so that we can use ml algorithm on it. Here we use the TfidVectorizer module to transform words into numerical data. Basically it counts the frequency of each words. We then produce the correponding labels of each data points to pair with the numerical data. 
</span><span class="s0">#%% 
</span><span class="s1">vectorizer = TfidfVectorizer(stop_words={</span><span class="s4">'english'</span><span class="s1">})</span>
<span class="s1">X = vectorizer.fit_transform(aggregated_df).toarray()</span>
<span class="s1">y = np.array(functools.reduce(</span><span class="s2">lambda </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b: a + b</span><span class="s2">, </span><span class="s1">[[names[i]] * lens[i + </span><span class="s3">1</span><span class="s1">] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(names))]))</span>
<span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">X.shape</span>
<span class="s0">#%% md 
</span><span class="s1">We will use PCA to visualize the data. See that the shape of X is (16936, 30112), and it will be impossible to plot such data. To maximize variance, the best we could do while visualizing data is to plot it as a 3D scatter plot. Therefore, we will choose ethe number of components to be three here. Then, we will apply the dimensionality reduction on it. 
</span><span class="s0">#%% 
</span><span class="s1">pca = PCA(n_components=</span><span class="s3">3</span><span class="s1">)</span>
<span class="s1">reduc_X = pca.fit_transform(X)</span>
<span class="s1">reduc_X</span>
<span class="s0">#%% md 
</span><span class="s1">Here, we simply calculate the prefix sum fo the length array to use it for indexing for plotting down the line. 
</span><span class="s0">#%% 
</span><span class="s1">clusters_means = []</span>
<span class="s1">clusters = []</span>
<span class="s1">prefix = lens.cumsum()</span>
<span class="s1">prefix</span>
<span class="s0">#%% md 
</span><span class="s1">Here we split each cluster here. 
</span><span class="s0">#%% 
</span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(lens.size - </span><span class="s3">1</span><span class="s1">):</span>
    <span class="s1">clusters.append(reduc_X[prefix[i]:prefix[i+</span><span class="s3">1</span><span class="s1">]</span><span class="s2">,</span><span class="s1">:])</span>
    <span class="s1">clusters_means.append(X[prefix[i]:prefix[i+</span><span class="s3">1</span><span class="s1">]</span><span class="s2">,</span><span class="s1">:].mean(axis=</span><span class="s3">0</span><span class="s1">))</span>

<span class="s1">clusters_means</span><span class="s2">, </span><span class="s1">clusters</span><span class="s2">, </span><span class="s1">len(clusters)</span>
<span class="s0">#%% md 
</span><span class="s1">We plot the projected data down by using the 3d scatter plot and label each clusters according to the news source here. Notice that some clusters like the bbc news are very easily separable from each other while some like everydayhealth and foxnews are harder to separate in 3 dimensions. However, it might be possible to find some projection in higher dimensional space such that it will separate each clusters from each other. 
</span><span class="s0">#%% 
</span><span class="s1">fig = plt.figure()</span>
<span class="s1">ax = fig.add_subplot(projection=</span><span class="s4">'3d'</span><span class="s1">)</span>
<span class="s1">colors = [</span><span class="s4">&quot;red&quot;</span><span class="s2">, </span><span class="s4">&quot;blue&quot;</span><span class="s2">, </span><span class="s4">&quot;green&quot;</span><span class="s2">, </span><span class="s4">&quot;purple&quot;</span><span class="s2">, </span><span class="s4">&quot;orange&quot;</span><span class="s1">]</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(clusters)):</span>
    <span class="s1">temp = clusters[i]</span>
    <span class="s1">ax.scatter(temp[:</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">temp[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">temp[:</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">c=colors[i]</span><span class="s2">, </span><span class="s1">label=names[i])</span>

<span class="s1">ax.legend(loc=</span><span class="s4">&quot;upper left&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">We can use tsne to visualize the data similarly here. However, since TSNE are very computationally expensive for data with higher number of features, we will use pca to project the data into the smaller subspace and only then perform tsne on the projected data to embed the projected data points to a 3D space for visualization again. Here we chose perplexity to be 40 because the dataset is large, simiarly for early_exacggeration. The intermidiate space that we project the data point to using the pca will have 100 dimensions. 
</span><span class="s0">#%% 
</span><span class="s2">from </span><span class="s1">sklearn.manifold </span><span class="s2">import </span><span class="s1">TSNE</span>
<span class="s1">tsne = TSNE(n_components=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">perplexity=</span><span class="s3">40</span><span class="s2">, </span><span class="s1">early_exaggeration=</span><span class="s3">50</span><span class="s1">)</span>
<span class="s1">pca = PCA(n_components=</span><span class="s3">100</span><span class="s1">)</span>
<span class="s1">first_step_reduction = pca.fit_transform(X)</span>
<span class="s1">reduc_X = tsne.fit_transform(first_step_reduction)</span>
<span class="s1">reduc_X</span>
<span class="s0">#%% md 
</span><span class="s1">The clusters here are pretty clump which is somewhat expected because the input matrix X is extremely sparse and the dimension is very high. So using PCA to visualize data is a better choice here. 
</span><span class="s0">#%% 
</span><span class="s1">clusters_means = []</span>
<span class="s1">clusters = []</span>
<span class="s1">prefix = lens.cumsum()</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(lens.size - </span><span class="s3">1</span><span class="s1">):</span>
    <span class="s1">clusters.append(reduc_X[prefix[i]:prefix[i+</span><span class="s3">1</span><span class="s1">]</span><span class="s2">,</span><span class="s1">:])</span>
    <span class="s1">clusters_means.append(X[prefix[i]:prefix[i+</span><span class="s3">1</span><span class="s1">]</span><span class="s2">,</span><span class="s1">:].mean(axis=</span><span class="s3">0</span><span class="s1">))</span>

<span class="s1">clusters_means</span><span class="s2">, </span><span class="s1">clusters</span><span class="s2">, </span><span class="s1">len(clusters)</span>

<span class="s1">fig = plt.figure()</span>
<span class="s1">ax = fig.add_subplot(projection=</span><span class="s4">'3d'</span><span class="s1">)</span>
<span class="s1">colors = [</span><span class="s4">&quot;red&quot;</span><span class="s2">, </span><span class="s4">&quot;blue&quot;</span><span class="s2">, </span><span class="s4">&quot;green&quot;</span><span class="s2">, </span><span class="s4">&quot;purple&quot;</span><span class="s2">, </span><span class="s4">&quot;orange&quot;</span><span class="s1">]</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(len(clusters)):</span>
    <span class="s1">temp = clusters[i]</span>
    <span class="s1">ax.scatter(temp[:</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">temp[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">temp[:</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">c=colors[i]</span><span class="s2">, </span><span class="s1">label=names[i])</span>

<span class="s1">ax.legend(loc=</span><span class="s4">&quot;upper left&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">Here we compute further statistic of data. Stds are computed for each basis. Its sum and mean of the stds of each feature are given below. As we can see, the mean of the standard deviation is very low ~ 0.00576, which implies that the data are very closely packed around the center becaus the min and the max are also around the same value. 
 
Although the matrix is very large, it is mostly zeroes as the sparsity is very high here (nearly 1) which implies that 99 percent of the entries are zeroes. 
In conclusion, the data has very high dimension however it is very sparse (99 percent zeroes), and the data points are tightly packed around the origin (0, 0, ..., 0). However, from the pca plot, we know that the data is separable. 
</span><span class="s0">#%% 
</span><span class="s1">stats = data_stats(X)</span>
<span class="s1">stats</span>
<span class="s0">#%% 
</span></pre>
</body>
</html>