{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Originally, I'm testing kmeans algorithm. The dataset I used here are raw text so we need to do some processing first. The function used to load and parse this dataset is in another file called health_news_parser which contains two function to load the data for testing for kmeans and also decision tree model. Since the raw data is text, we need to apply the TDIF feature extractor to convert words into frequency which is a numerical data that can be consumed by the ML algorithms. Since the number of features is high ~ 30000 ish, and the data is very sparse which is around ~ 0.99, it is impossible to visualize how each feature affect the hypothesis at this point. We will now skip to the analysis part after applying dimensionality reduction and collecting the result. The results load into this notebook are output from code in another file.\n",
    "\n",
    "We will explore the importance of each features later after training the decision tree."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:28.786157Z",
     "start_time": "2023-07-05T04:41:28.768079Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from health_news_parser import load_health_news_for_decision_tree\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "path = \"../output/decision_tree/\"\n",
    "df_each = []\n",
    "for filename in os.listdir(path):\n",
    "    f = os.path.join(path, filename)\n",
    "    if os.path.isfile(f) and (\"2\" in f or \"5\" in f):\n",
    "        temp = pd.read_csv(f)\n",
    "        temp = temp[temp.name.apply(lambda x: x != \"name\")]\n",
    "        df_each.append((temp, filename))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:28.786317Z",
     "start_time": "2023-07-05T04:41:28.772012Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def score(reduc_time, accuracy, train_time, red_w = 200, ac_w = 1, train_w = 20):\n",
    "    return (accuracy ** ac_w)*(red_w/int(reduc_time + 1) + train_w/int(train_time + 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:28.786355Z",
     "start_time": "2023-07-05T04:41:28.780099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "red_w = 10\n",
    "ac_w = 10\n",
    "train_w = 10\n",
    "included_cols = [\"name\", \"filename\", \"original_shape\", \"transformed_shape\", \"params\", \"reduction_time\", \"accuracy\", \"train_time\", \"score_series\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:28.789910Z",
     "start_time": "2023-07-05T04:41:28.786030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(columns=included_cols)\n",
    "collected_all_df = pd.DataFrame(columns=included_cols)\n",
    "for res in df_each:\n",
    "    res_copy = res[0].copy()\n",
    "    res_copy[\"filename\"] = res[1]\n",
    "    score_series = res_copy[res_copy.name != \"Nothing\"][[\"reduction_time\", \"train_time\", \"accuracy\"]].apply(lambda x: score(x[\"reduction_time\"], x[\"accuracy\"], red_w, ac_w, train_w), axis=1)\n",
    "    res_copy_score = res_copy.copy()\n",
    "    res_copy_score[\"score_series\"] = score_series\n",
    "    out_df = pd.concat([out_df, res_copy_score])\n",
    "    max_ind = res_copy_score.groupby(by=[\"original_shape\"])[\"score_series\"].idxmax()\n",
    "    collected = res_copy_score.iloc[max_ind][[\"name\", \"filename\", \"original_shape\", \"transformed_shape\", \"params\", \"reduction_time\", \"accuracy\", \"train_time\", \"score_series\"]]\n",
    "    collected_all_df = pd.concat([collected_all_df, collected])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:28.800360Z",
     "start_time": "2023-07-05T04:41:28.791360Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are the results of each dimensionality algorithm. One of the columns is the column for descrbing accuracy. As we can see here, for the decision tree model, the sparse JL transform performs the best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      name  original_shape transformed_shape  \\\nfilename                                                                       \n2        0   extremely sparse JL transform   (7521, 15826)      (7721, 4041)   \n         1   extremely sparse JL transform   (7521, 15826)      (7721, 3907)   \n         2   extremely sparse JL transform   (7521, 15826)      (7721, 2020)   \n         3   extremely sparse JL transform   (7521, 15826)      (7721, 1953)   \n         4   extremely sparse JL transform   (7521, 15826)       (7721, 404)   \n         5   extremely sparse JL transform   (7521, 15826)       (7721, 390)   \n         6   extremely sparse JL transform   (7521, 15826)       (7721, 224)   \n         7   extremely sparse JL transform   (7521, 15826)       (7721, 217)   \n         8             sparse JL transform   (7521, 15826)      (7721, 1582)   \n         9             sparse JL transform   (7521, 15826)      (7721, 1582)   \n         10            sparse JL transform   (7521, 15826)      (7721, 7189)   \n         11            sparse JL transform   (7521, 15826)      (7721, 5526)   \n         12            sparse JL transform   (7521, 15826)       (7721, 287)   \n         13            sparse JL transform   (7521, 15826)       (7721, 221)   \n         14            sparse JL transform   (7521, 15826)        (7721, 88)   \n         15            sparse JL transform   (7521, 15826)        (7721, 68)   \n         16                   JL transform   (7521, 15826)      (7721, 1582)   \n         17                   JL transform   (7521, 15826)      (7721, 1582)   \n         18                   JL transform   (7521, 15826)      (7721, 7189)   \n         19                   JL transform   (7521, 15826)      (7721, 5526)   \n         20                   JL transform   (7521, 15826)       (7721, 287)   \n         21                   JL transform   (7521, 15826)       (7721, 221)   \n         22                   JL transform   (7521, 15826)        (7721, 88)   \n         23                   JL transform   (7521, 15826)        (7721, 68)   \n         24                            PCA   (7521, 15826)        (7721, 77)   \n         25                        Nothing   (7521, 15826)     (7721, 15826)   \n5        0   extremely sparse JL transform  (18920, 34042)     (19420, 4746)   \n         1   extremely sparse JL transform  (18920, 34042)     (19420, 4602)   \n         2   extremely sparse JL transform  (18920, 34042)     (19420, 2373)   \n         3   extremely sparse JL transform  (18920, 34042)     (19420, 2301)   \n         4   extremely sparse JL transform  (18920, 34042)      (19420, 474)   \n         5   extremely sparse JL transform  (18920, 34042)      (19420, 460)   \n         6   extremely sparse JL transform  (18920, 34042)      (19420, 263)   \n         7   extremely sparse JL transform  (18920, 34042)      (19420, 255)   \n         8             sparse JL transform  (18920, 34042)    (19420, 28759)   \n         9             sparse JL transform  (18920, 34042)    (19420, 22104)   \n         10            sparse JL transform  (18920, 34042)     (19420, 7189)   \n         11            sparse JL transform  (18920, 34042)     (19420, 5526)   \n         12            sparse JL transform  (18920, 34042)      (19420, 287)   \n         13            sparse JL transform  (18920, 34042)      (19420, 221)   \n         14            sparse JL transform  (18920, 34042)       (19420, 88)   \n         15            sparse JL transform  (18920, 34042)       (19420, 68)   \n         16                   JL transform  (18920, 34042)    (19420, 28759)   \n         17                   JL transform  (18920, 34042)    (19420, 22104)   \n         18                   JL transform  (18920, 34042)     (19420, 7189)   \n         19                   JL transform  (18920, 34042)     (19420, 5526)   \n         20                   JL transform  (18920, 34042)      (19420, 287)   \n         21                   JL transform  (18920, 34042)      (19420, 221)   \n         22                   JL transform  (18920, 34042)       (19420, 88)   \n         23                   JL transform  (18920, 34042)       (19420, 68)   \n         24                            PCA  (18920, 34042)      (19420, 194)   \n         25                        Nothing  (18920, 34042)    (19420, 34042)   \n\n                                                  params  reduction_time  \\\nfilename                                                                   \n2        0                      {'ep': 0.05, 'de': 0.05}        0.499629   \n         1                       {'ep': 0.05, 'de': 0.1}        0.556030   \n         2                       {'ep': 0.1, 'de': 0.05}        0.488768   \n         3                        {'ep': 0.1, 'de': 0.1}        0.466752   \n         4                       {'ep': 0.5, 'de': 0.05}        0.467465   \n         5                        {'ep': 0.5, 'de': 0.1}        0.462496   \n         6                       {'ep': 0.9, 'de': 0.05}        0.462231   \n         7                        {'ep': 0.9, 'de': 0.1}        0.462833   \n         8                      {'ep': 0.05, 'de': 0.05}        2.004327   \n         9                       {'ep': 0.05, 'de': 0.1}        1.984776   \n         10                      {'ep': 0.1, 'de': 0.05}        7.600143   \n         11                       {'ep': 0.1, 'de': 0.1}        5.803091   \n         12                      {'ep': 0.5, 'de': 0.05}        0.510463   \n         13                       {'ep': 0.5, 'de': 0.1}        0.504962   \n         14                      {'ep': 0.9, 'de': 0.05}        0.285085   \n         15                       {'ep': 0.9, 'de': 0.1}        0.285990   \n         16                     {'ep': 0.05, 'de': 0.05}        1.875658   \n         17                      {'ep': 0.05, 'de': 0.1}        1.879715   \n         18                      {'ep': 0.1, 'de': 0.05}        7.641705   \n         19                       {'ep': 0.1, 'de': 0.1}        5.689541   \n         20                      {'ep': 0.5, 'de': 0.05}        0.487102   \n         21                       {'ep': 0.5, 'de': 0.1}        0.452248   \n         22                      {'ep': 0.9, 'de': 0.05}        0.264045   \n         23                       {'ep': 0.9, 'de': 0.1}        0.209704   \n         24   {'n_components': 77, 'svd_solver': 'auto'}        4.410252   \n         25                                           {}        0.000198   \n5        0                      {'ep': 0.05, 'de': 0.05}        5.082444   \n         1                       {'ep': 0.05, 'de': 0.1}        5.141505   \n         2                       {'ep': 0.1, 'de': 0.05}        4.712962   \n         3                        {'ep': 0.1, 'de': 0.1}        4.155604   \n         4                       {'ep': 0.5, 'de': 0.05}        4.018365   \n         5                        {'ep': 0.5, 'de': 0.1}        3.978315   \n         6                       {'ep': 0.9, 'de': 0.05}        3.838281   \n         7                        {'ep': 0.9, 'de': 0.1}        3.829837   \n         8                      {'ep': 0.05, 'de': 0.05}      141.830719   \n         9                       {'ep': 0.05, 'de': 0.1}      105.447106   \n         10                      {'ep': 0.1, 'de': 0.05}       32.969807   \n         11                       {'ep': 0.1, 'de': 0.1}       25.320903   \n         12                      {'ep': 0.5, 'de': 0.05}        2.270777   \n         13                       {'ep': 0.5, 'de': 0.1}        2.091690   \n         14                      {'ep': 0.9, 'de': 0.05}        1.570569   \n         15                       {'ep': 0.9, 'de': 0.1}        1.766735   \n         16                     {'ep': 0.05, 'de': 0.05}      128.494039   \n         17                      {'ep': 0.05, 'de': 0.1}      101.825684   \n         18                      {'ep': 0.1, 'de': 0.05}       33.780918   \n         19                       {'ep': 0.1, 'de': 0.1}       25.169705   \n         20                      {'ep': 0.5, 'de': 0.05}        2.276009   \n         21                       {'ep': 0.5, 'de': 0.1}        2.022318   \n         22                      {'ep': 0.9, 'de': 0.05}        1.411484   \n         23                       {'ep': 0.9, 'de': 0.1}        1.696405   \n         24  {'n_components': 194, 'svd_solver': 'auto'}       39.313781   \n         25                                           {}        0.000457   \n\n             accuracy  train_time  score_series  \nfilename                                         \n2        0      0.990    4.184810     10.688152  \n         1      0.935    6.166727      6.034854  \n         2      0.765    4.725257      0.811269  \n         3      0.960    2.924579      7.857113  \n         4      0.625    1.888037      0.107486  \n         5      0.620    2.470510      0.099190  \n         6      0.600    1.309810      0.071460  \n         7      0.585    1.009665      0.055477  \n         8      0.980    3.095135      4.209163  \n         9      0.985    3.169277      5.861798  \n         10     0.980    4.187765      2.506928  \n         11     0.985    3.866942      2.996030  \n         12     0.980    2.807974      9.656315  \n         13     0.965    2.698205      8.276063  \n         14     0.915    2.385611      4.861403  \n         15     0.895    2.297592      3.897454  \n         16     0.870   14.673104      1.693796  \n         17     0.925   14.589144      3.126698  \n         18     0.960   33.292576      2.039827  \n         19     0.940   29.221159      1.876992  \n         20     0.855    5.833775      2.467242  \n         21     0.840    5.065625      2.067015  \n         22     0.765    3.363958      0.811269  \n         23     0.735    2.968241      0.543781  \n         24     0.985    2.446190      3.282607  \n         25     0.985   15.881628           NaN  \n5        0      0.716   33.192836      0.123400  \n         1      0.602   39.720911      0.021785  \n         2      0.816   14.426370      0.499756  \n         3      0.422   32.232944      0.000684  \n         4      0.300    7.455794      0.000023  \n         5      0.254    9.169791      0.000005  \n         6      0.264    3.908615      0.000007  \n         7      0.242    4.250398      0.000003  \n         8      0.982   32.052107      1.574910  \n         9      0.978   28.716505      1.531069  \n         10     0.968   19.986369      1.532278  \n         11     0.968   19.297996      1.591212  \n         12     0.926   10.262342      2.388058  \n         13     0.926    9.428983      2.388058  \n         14     0.850    7.869968      1.342325  \n         15     0.828    7.453368      1.032695  \n         16     0.800  202.560610      0.203549  \n         17     0.804  177.586481      0.216275  \n         18     0.784  100.200348      0.185317  \n         19     0.744   88.124119      0.114473  \n         20     0.644   19.398569      0.063211  \n         21     0.560   17.071901      0.015625  \n         22     0.514   11.263186      0.008776  \n         23     0.484   10.120562      0.004810  \n         24     0.958   15.654374      1.346616  \n         25     0.978  107.896455           NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>name</th>\n      <th>original_shape</th>\n      <th>transformed_shape</th>\n      <th>params</th>\n      <th>reduction_time</th>\n      <th>accuracy</th>\n      <th>train_time</th>\n      <th>score_series</th>\n    </tr>\n    <tr>\n      <th>filename</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"26\" valign=\"top\">2</th>\n      <th>0</th>\n      <td>extremely sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 4041)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>0.499629</td>\n      <td>0.990</td>\n      <td>4.184810</td>\n      <td>10.688152</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>extremely sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 3907)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>0.556030</td>\n      <td>0.935</td>\n      <td>6.166727</td>\n      <td>6.034854</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>extremely sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 2020)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>0.488768</td>\n      <td>0.765</td>\n      <td>4.725257</td>\n      <td>0.811269</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>extremely sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 1953)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>0.466752</td>\n      <td>0.960</td>\n      <td>2.924579</td>\n      <td>7.857113</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>extremely sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 404)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>0.467465</td>\n      <td>0.625</td>\n      <td>1.888037</td>\n      <td>0.107486</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>extremely sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 390)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>0.462496</td>\n      <td>0.620</td>\n      <td>2.470510</td>\n      <td>0.099190</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>extremely sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 224)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>0.462231</td>\n      <td>0.600</td>\n      <td>1.309810</td>\n      <td>0.071460</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>extremely sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 217)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>0.462833</td>\n      <td>0.585</td>\n      <td>1.009665</td>\n      <td>0.055477</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 1582)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>2.004327</td>\n      <td>0.980</td>\n      <td>3.095135</td>\n      <td>4.209163</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 1582)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>1.984776</td>\n      <td>0.985</td>\n      <td>3.169277</td>\n      <td>5.861798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 7189)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>7.600143</td>\n      <td>0.980</td>\n      <td>4.187765</td>\n      <td>2.506928</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 5526)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>5.803091</td>\n      <td>0.985</td>\n      <td>3.866942</td>\n      <td>2.996030</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 287)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>0.510463</td>\n      <td>0.980</td>\n      <td>2.807974</td>\n      <td>9.656315</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 221)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>0.504962</td>\n      <td>0.965</td>\n      <td>2.698205</td>\n      <td>8.276063</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 88)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>0.285085</td>\n      <td>0.915</td>\n      <td>2.385611</td>\n      <td>4.861403</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>sparse JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 68)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>0.285990</td>\n      <td>0.895</td>\n      <td>2.297592</td>\n      <td>3.897454</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 1582)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>1.875658</td>\n      <td>0.870</td>\n      <td>14.673104</td>\n      <td>1.693796</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 1582)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>1.879715</td>\n      <td>0.925</td>\n      <td>14.589144</td>\n      <td>3.126698</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 7189)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>7.641705</td>\n      <td>0.960</td>\n      <td>33.292576</td>\n      <td>2.039827</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 5526)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>5.689541</td>\n      <td>0.940</td>\n      <td>29.221159</td>\n      <td>1.876992</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 287)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>0.487102</td>\n      <td>0.855</td>\n      <td>5.833775</td>\n      <td>2.467242</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 221)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>0.452248</td>\n      <td>0.840</td>\n      <td>5.065625</td>\n      <td>2.067015</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 88)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>0.264045</td>\n      <td>0.765</td>\n      <td>3.363958</td>\n      <td>0.811269</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>JL transform</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 68)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>0.209704</td>\n      <td>0.735</td>\n      <td>2.968241</td>\n      <td>0.543781</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>PCA</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 77)</td>\n      <td>{'n_components': 77, 'svd_solver': 'auto'}</td>\n      <td>4.410252</td>\n      <td>0.985</td>\n      <td>2.446190</td>\n      <td>3.282607</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Nothing</td>\n      <td>(7521, 15826)</td>\n      <td>(7721, 15826)</td>\n      <td>{}</td>\n      <td>0.000198</td>\n      <td>0.985</td>\n      <td>15.881628</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"26\" valign=\"top\">5</th>\n      <th>0</th>\n      <td>extremely sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 4746)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>5.082444</td>\n      <td>0.716</td>\n      <td>33.192836</td>\n      <td>0.123400</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>extremely sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 4602)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>5.141505</td>\n      <td>0.602</td>\n      <td>39.720911</td>\n      <td>0.021785</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>extremely sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 2373)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>4.712962</td>\n      <td>0.816</td>\n      <td>14.426370</td>\n      <td>0.499756</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>extremely sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 2301)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>4.155604</td>\n      <td>0.422</td>\n      <td>32.232944</td>\n      <td>0.000684</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>extremely sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 474)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>4.018365</td>\n      <td>0.300</td>\n      <td>7.455794</td>\n      <td>0.000023</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>extremely sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 460)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>3.978315</td>\n      <td>0.254</td>\n      <td>9.169791</td>\n      <td>0.000005</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>extremely sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 263)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>3.838281</td>\n      <td>0.264</td>\n      <td>3.908615</td>\n      <td>0.000007</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>extremely sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 255)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>3.829837</td>\n      <td>0.242</td>\n      <td>4.250398</td>\n      <td>0.000003</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 28759)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>141.830719</td>\n      <td>0.982</td>\n      <td>32.052107</td>\n      <td>1.574910</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 22104)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>105.447106</td>\n      <td>0.978</td>\n      <td>28.716505</td>\n      <td>1.531069</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 7189)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>32.969807</td>\n      <td>0.968</td>\n      <td>19.986369</td>\n      <td>1.532278</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 5526)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>25.320903</td>\n      <td>0.968</td>\n      <td>19.297996</td>\n      <td>1.591212</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 287)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>2.270777</td>\n      <td>0.926</td>\n      <td>10.262342</td>\n      <td>2.388058</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 221)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>2.091690</td>\n      <td>0.926</td>\n      <td>9.428983</td>\n      <td>2.388058</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 88)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>1.570569</td>\n      <td>0.850</td>\n      <td>7.869968</td>\n      <td>1.342325</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>sparse JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 68)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>1.766735</td>\n      <td>0.828</td>\n      <td>7.453368</td>\n      <td>1.032695</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 28759)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>128.494039</td>\n      <td>0.800</td>\n      <td>202.560610</td>\n      <td>0.203549</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 22104)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>101.825684</td>\n      <td>0.804</td>\n      <td>177.586481</td>\n      <td>0.216275</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 7189)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>33.780918</td>\n      <td>0.784</td>\n      <td>100.200348</td>\n      <td>0.185317</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 5526)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>25.169705</td>\n      <td>0.744</td>\n      <td>88.124119</td>\n      <td>0.114473</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 287)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>2.276009</td>\n      <td>0.644</td>\n      <td>19.398569</td>\n      <td>0.063211</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 221)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>2.022318</td>\n      <td>0.560</td>\n      <td>17.071901</td>\n      <td>0.015625</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 88)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>1.411484</td>\n      <td>0.514</td>\n      <td>11.263186</td>\n      <td>0.008776</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>JL transform</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 68)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>1.696405</td>\n      <td>0.484</td>\n      <td>10.120562</td>\n      <td>0.004810</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>PCA</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 194)</td>\n      <td>{'n_components': 194, 'svd_solver': 'auto'}</td>\n      <td>39.313781</td>\n      <td>0.958</td>\n      <td>15.654374</td>\n      <td>1.346616</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Nothing</td>\n      <td>(18920, 34042)</td>\n      <td>(19420, 34042)</td>\n      <td>{}</td>\n      <td>0.000457</td>\n      <td>0.978</td>\n      <td>107.896455</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = out_df.groupby(by=\"filename\", group_keys=True).apply(lambda x: x[:])\n",
    "all[[\"name\", \"original_shape\", \"transformed_shape\", \"params\", \"reduction_time\", \"accuracy\", \"train_time\", \"score_series\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:28.809190Z",
     "start_time": "2023-07-05T04:41:28.799328Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        original_shape  transformed_shape  \\\nfilename name                                                               \n2        extremely sparse JL transform               1                  1   \n5        sparse JL transform                         1                  1   \n\n                                        params  reduction_time  accuracy  \\\nfilename name                                                              \n2        extremely sparse JL transform       1               1         1   \n5        sparse JL transform                 1               1         1   \n\n                                        train_time  score_series  \nfilename name                                                     \n2        extremely sparse JL transform           1             1  \n5        sparse JL transform                     1             1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>original_shape</th>\n      <th>transformed_shape</th>\n      <th>params</th>\n      <th>reduction_time</th>\n      <th>accuracy</th>\n      <th>train_time</th>\n      <th>score_series</th>\n    </tr>\n    <tr>\n      <th>filename</th>\n      <th>name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <th>extremely sparse JL transform</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <th>sparse JL transform</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_all_df.groupby([\"filename\", \"name\"]).count().sort_values(by=\"original_shape\").groupby(level=0).tail(1).sort_values(by=\"filename\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:28.817377Z",
     "start_time": "2023-07-05T04:41:28.809575Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['00', '000', '0000', ..., 'zzukaz', 'zzzzz', 'zâ'], dtype=object)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, names = load_health_news_for_decision_tree(5, \"../data/health+news+in+twitter/Health-Tweets/\")\n",
    "names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:31.471733Z",
     "start_time": "2023-07-05T04:41:28.817436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= data.training_data, data.testing_data, data.training_label, data.testing_label\n",
    "rf = RandomForestClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:41:31.485801Z",
     "start_time": "2023-07-05T04:41:31.482452Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier()",
      "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T04:43:16.804503Z",
     "start_time": "2023-07-05T04:41:31.486036Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "00         5.354939e-06\n000        1.787731e-04\n0000       1.376693e-07\n000th      3.023953e-05\n007        1.352438e-07\n               ...     \nzzgrrgf    5.969835e-08\nzznvii     0.000000e+00\nzzukaz     1.807494e-07\nzzzzz      1.945336e-08\nzâ         9.562023e-08\nLength: 34042, dtype: float64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.Series(rf.feature_importances_, index=names)\n",
    "std = np.std([importances for tree in rf.estimators_], axis=0)\n",
    "importances"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T05:08:27.784402Z",
     "start_time": "2023-07-05T05:08:27.753231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in      0.077884\n",
      "news    0.074664\n",
      "khne    0.059068\n",
      "ws      0.049009\n",
      "bit     0.038127\n",
      "to      0.037207\n",
      "com     0.037050\n",
      "ly      0.032531\n",
      "http    0.030724\n",
      "at      0.026368\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(importances.sort_values(ascending=False).iloc[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T05:08:29.068245Z",
     "start_time": "2023-07-05T05:08:29.058952Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are the most important features (words) that can be used to classify each news site."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_im = pd.Series(importances).sort_values(ascending=False).iloc[:10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "f_im.plot.bar(ax= ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T05:10:11.580188Z",
     "start_time": "2023-07-05T05:10:11.503723Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "path = \"../output/news/\"\n",
    "df_each = []\n",
    "for filename in os.listdir(path):\n",
    "    f = os.path.join(path, filename)\n",
    "    if os.path.isfile(f) and (\"2\" in f or \"5\" in f):\n",
    "        temp = pd.read_csv(f)\n",
    "        temp = temp[temp.name.apply(lambda x: x != \"name\")]\n",
    "        df_each.append((temp, filename))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T07:28:08.272203Z",
     "start_time": "2023-07-05T07:28:08.262018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(columns=included_cols)\n",
    "collected_all_df = pd.DataFrame(columns=included_cols)\n",
    "for res in df_each:\n",
    "    res_copy = res[0].copy()\n",
    "    res_copy[\"filename\"] = res[1]\n",
    "    score_series = res_copy[res_copy.name != \"Nothing\"][[\"reduction_time\", \"train_time\", \"accuracy\"]].apply(lambda x: score(x[\"reduction_time\"], x[\"accuracy\"], red_w, ac_w, train_w), axis=1)\n",
    "    res_copy_score = res_copy.copy()\n",
    "    res_copy_score[\"score_series\"] = score_series\n",
    "    out_df = pd.concat([out_df, res_copy_score])\n",
    "    max_ind = res_copy_score.groupby(by=[\"original_shape\"])[\"score_series\"].idxmax()\n",
    "    collected = res_copy_score.iloc[max_ind][[\"name\", \"filename\", \"original_shape\", \"transformed_shape\", \"params\", \"reduction_time\", \"accuracy\", \"train_time\", \"score_series\"]]\n",
    "    collected_all_df = pd.concat([collected_all_df, collected])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T07:28:08.995064Z",
     "start_time": "2023-07-05T07:28:08.991513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      name  original_shape transformed_shape  \\\nfilename                                                                       \n2        0   extremely sparse JL transform   (6726, 16990)      (6928, 4029)   \n         1   extremely sparse JL transform   (6726, 16990)      (6928, 3894)   \n         2   extremely sparse JL transform   (6726, 16990)      (6928, 2014)   \n         3   extremely sparse JL transform   (6726, 16990)      (6928, 1947)   \n         4   extremely sparse JL transform   (6726, 16990)       (6928, 402)   \n         5   extremely sparse JL transform   (6726, 16990)       (6928, 389)   \n         6   extremely sparse JL transform   (6726, 16990)       (6928, 223)   \n         7   extremely sparse JL transform   (6726, 16990)       (6928, 216)   \n         8             sparse JL transform   (6726, 16990)      (6928, 1699)   \n         9             sparse JL transform   (6726, 16990)      (6928, 1699)   \n         10            sparse JL transform   (6726, 16990)      (6928, 7189)   \n         11            sparse JL transform   (6726, 16990)      (6928, 5526)   \n         12            sparse JL transform   (6726, 16990)       (6928, 287)   \n         13            sparse JL transform   (6726, 16990)       (6928, 221)   \n         14            sparse JL transform   (6726, 16990)        (6928, 88)   \n         15            sparse JL transform   (6726, 16990)        (6928, 68)   \n         16                   JL transform   (6726, 16990)      (6928, 1699)   \n         17                   JL transform   (6726, 16990)      (6928, 1699)   \n         18                   JL transform   (6726, 16990)      (6928, 7189)   \n         19                   JL transform   (6726, 16990)      (6928, 5526)   \n         20                   JL transform   (6726, 16990)       (6928, 287)   \n         21                   JL transform   (6726, 16990)       (6928, 221)   \n         22                   JL transform   (6726, 16990)        (6928, 88)   \n         23                   JL transform   (6726, 16990)        (6928, 68)   \n         24                            PCA   (6726, 16990)        (6928, 69)   \n         25                        Nothing   (6726, 16990)     (6928, 16990)   \n5        0   extremely sparse JL transform  (23183, 40537)     (23688, 4910)   \n         1   extremely sparse JL transform  (23183, 40537)     (23688, 4763)   \n         2   extremely sparse JL transform  (23183, 40537)     (23688, 2455)   \n         3   extremely sparse JL transform  (23183, 40537)     (23688, 2381)   \n         4   extremely sparse JL transform  (23183, 40537)      (23688, 491)   \n         5   extremely sparse JL transform  (23183, 40537)      (23688, 476)   \n         6   extremely sparse JL transform  (23183, 40537)      (23688, 272)   \n         7   extremely sparse JL transform  (23183, 40537)      (23688, 264)   \n         8             sparse JL transform  (23183, 40537)    (23688, 28759)   \n         9             sparse JL transform  (23183, 40537)    (23688, 22104)   \n         10            sparse JL transform  (23183, 40537)     (23688, 7189)   \n         11            sparse JL transform  (23183, 40537)     (23688, 5526)   \n         12            sparse JL transform  (23183, 40537)      (23688, 287)   \n         13            sparse JL transform  (23183, 40537)      (23688, 221)   \n         14            sparse JL transform  (23183, 40537)       (23688, 88)   \n         15            sparse JL transform  (23183, 40537)       (23688, 68)   \n         16                   JL transform  (23183, 40537)    (23688, 28759)   \n         17                   JL transform  (23183, 40537)    (23688, 22104)   \n         18                   JL transform  (23183, 40537)     (23688, 7189)   \n         19                   JL transform  (23183, 40537)     (23688, 5526)   \n         20                   JL transform  (23183, 40537)      (23688, 287)   \n         21                   JL transform  (23183, 40537)      (23688, 221)   \n         22                   JL transform  (23183, 40537)       (23688, 88)   \n         23                   JL transform  (23183, 40537)       (23688, 68)   \n         24                            PCA  (23183, 40537)      (23688, 236)   \n         25                        Nothing  (23183, 40537)    (23688, 40537)   \n\n                                                  params  reduction_time  \\\nfilename                                                                   \n2        0                      {'ep': 0.05, 'de': 0.05}        0.437593   \n         1                       {'ep': 0.05, 'de': 0.1}        0.423221   \n         2                       {'ep': 0.1, 'de': 0.05}        0.408222   \n         3                        {'ep': 0.1, 'de': 0.1}        0.411041   \n         4                       {'ep': 0.5, 'de': 0.05}        0.408369   \n         5                        {'ep': 0.5, 'de': 0.1}        0.406945   \n         6                       {'ep': 0.9, 'de': 0.05}        0.405903   \n         7                        {'ep': 0.9, 'de': 0.1}        0.412853   \n         8                      {'ep': 0.05, 'de': 0.05}        2.578185   \n         9                       {'ep': 0.05, 'de': 0.1}        2.417723   \n         10                      {'ep': 0.1, 'de': 0.05}        8.317118   \n         11                       {'ep': 0.1, 'de': 0.1}        6.398866   \n         12                      {'ep': 0.5, 'de': 0.05}        0.783363   \n         13                       {'ep': 0.5, 'de': 0.1}        0.824684   \n         14                      {'ep': 0.9, 'de': 0.05}        0.559807   \n         15                       {'ep': 0.9, 'de': 0.1}        0.302327   \n         16                     {'ep': 0.05, 'de': 0.05}        2.212834   \n         17                      {'ep': 0.05, 'de': 0.1}        2.079176   \n         18                      {'ep': 0.1, 'de': 0.05}        7.440061   \n         19                       {'ep': 0.1, 'de': 0.1}        6.138703   \n         20                      {'ep': 0.5, 'de': 0.05}        0.643513   \n         21                       {'ep': 0.5, 'de': 0.1}        0.599054   \n         22                      {'ep': 0.9, 'de': 0.05}        0.386148   \n         23                       {'ep': 0.9, 'de': 0.1}        0.329178   \n         24   {'n_components': 69, 'svd_solver': 'auto'}        5.966820   \n         25                                           {}        0.000010   \n5        0                      {'ep': 0.05, 'de': 0.05}        8.357938   \n         1                       {'ep': 0.05, 'de': 0.1}        8.129092   \n         2                       {'ep': 0.1, 'de': 0.05}        8.239602   \n         3                        {'ep': 0.1, 'de': 0.1}        8.126419   \n         4                       {'ep': 0.5, 'de': 0.05}        8.022683   \n         5                        {'ep': 0.5, 'de': 0.1}        7.733724   \n         6                       {'ep': 0.9, 'de': 0.05}        7.717745   \n         7                        {'ep': 0.9, 'de': 0.1}        7.519420   \n         8                      {'ep': 0.05, 'de': 0.05}      198.481353   \n         9                       {'ep': 0.05, 'de': 0.1}      154.797251   \n         10                      {'ep': 0.1, 'de': 0.05}       54.859835   \n         11                       {'ep': 0.1, 'de': 0.1}       40.870175   \n         12                      {'ep': 0.5, 'de': 0.05}        4.095488   \n         13                       {'ep': 0.5, 'de': 0.1}        3.690838   \n         14                      {'ep': 0.9, 'de': 0.05}        2.716613   \n         15                       {'ep': 0.9, 'de': 0.1}        2.602714   \n         16                     {'ep': 0.05, 'de': 0.05}      188.893031   \n         17                      {'ep': 0.05, 'de': 0.1}      137.989284   \n         18                      {'ep': 0.1, 'de': 0.05}       45.495060   \n         19                       {'ep': 0.1, 'de': 0.1}       34.895092   \n         20                      {'ep': 0.5, 'de': 0.05}        3.346592   \n         21                       {'ep': 0.5, 'de': 0.1}        3.132158   \n         22                      {'ep': 0.9, 'de': 0.05}        2.641303   \n         23                       {'ep': 0.9, 'de': 0.1}        2.438814   \n         24  {'n_components': 236, 'svd_solver': 'auto'}       60.950643   \n         25                                           {}        0.000038   \n\n             accuracy  train_time  score_series  \nfilename                                         \n2        0      1.000    0.906473     11.818182  \n         1      0.935    0.715884      6.034854  \n         2      0.975    0.563305      9.174805  \n         3      1.000    0.475363     11.818182  \n         4      0.975    0.544050      9.174805  \n         5      1.000    0.472049     11.818182  \n         6      0.530    0.434989      0.020669  \n         7      0.950    0.369506      7.075982  \n         8      0.885    0.592594      1.518335  \n         9      0.985    0.589766      4.428914  \n         10     0.915    0.876677      1.204963  \n         11     0.590    0.737288      0.016595  \n         12     0.685    0.613112      0.268817  \n         13     0.695    0.559283      0.310741  \n         14     0.510    0.507711      0.014069  \n         15     0.570    0.478668      0.042786  \n         16     0.665    0.541991      0.087127  \n         17     0.890    0.555577      1.606331  \n         18     0.910    0.986320      1.194799  \n         19     0.905    0.820940      1.196562  \n         20     0.780    0.585150      0.985137  \n         21     0.605    0.432916      0.077643  \n         22     0.645    0.489784      0.147281  \n         23     0.675    0.453571      0.232054  \n         24     0.900    0.496544      1.215092  \n         25     0.725    1.589015           NaN  \n5        0      0.638    1.393794      0.032732  \n         1      0.374    1.204843      0.000157  \n         2      0.524    0.820879      0.004572  \n         3      0.954    0.790365      1.829138  \n         4      0.936    0.607731      1.511894  \n         5      0.886    0.519516      0.914573  \n         6      0.954    0.614072      1.915865  \n         7      0.902    0.641219      1.093822  \n         8      0.680    4.324042      0.039497  \n         9      0.604    3.471630      0.012166  \n         10     0.576    1.531134      0.008040  \n         11     0.742    1.055391      0.104315  \n         12     0.436    0.580237      0.000948  \n         13     0.460    0.502905      0.001832  \n         14     0.246    0.530785      0.000004  \n         15     0.278    0.451527      0.000014  \n         16     0.600    5.375067      0.011314  \n         17     0.590    4.317272      0.009663  \n         18     0.590    1.593343      0.010404  \n         19     0.624    1.322746      0.018831  \n         20     0.456    0.634654      0.001679  \n         21     0.358    0.581430      0.000149  \n         22     0.318    0.536496      0.000054  \n         23     0.306    0.541322      0.000037  \n         24     0.690    0.578181      0.048486  \n         25     0.622    7.140457           NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>name</th>\n      <th>original_shape</th>\n      <th>transformed_shape</th>\n      <th>params</th>\n      <th>reduction_time</th>\n      <th>accuracy</th>\n      <th>train_time</th>\n      <th>score_series</th>\n    </tr>\n    <tr>\n      <th>filename</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"26\" valign=\"top\">2</th>\n      <th>0</th>\n      <td>extremely sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 4029)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>0.437593</td>\n      <td>1.000</td>\n      <td>0.906473</td>\n      <td>11.818182</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>extremely sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 3894)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>0.423221</td>\n      <td>0.935</td>\n      <td>0.715884</td>\n      <td>6.034854</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>extremely sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 2014)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>0.408222</td>\n      <td>0.975</td>\n      <td>0.563305</td>\n      <td>9.174805</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>extremely sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 1947)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>0.411041</td>\n      <td>1.000</td>\n      <td>0.475363</td>\n      <td>11.818182</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>extremely sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 402)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>0.408369</td>\n      <td>0.975</td>\n      <td>0.544050</td>\n      <td>9.174805</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>extremely sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 389)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>0.406945</td>\n      <td>1.000</td>\n      <td>0.472049</td>\n      <td>11.818182</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>extremely sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 223)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>0.405903</td>\n      <td>0.530</td>\n      <td>0.434989</td>\n      <td>0.020669</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>extremely sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 216)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>0.412853</td>\n      <td>0.950</td>\n      <td>0.369506</td>\n      <td>7.075982</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 1699)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>2.578185</td>\n      <td>0.885</td>\n      <td>0.592594</td>\n      <td>1.518335</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 1699)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>2.417723</td>\n      <td>0.985</td>\n      <td>0.589766</td>\n      <td>4.428914</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 7189)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>8.317118</td>\n      <td>0.915</td>\n      <td>0.876677</td>\n      <td>1.204963</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 5526)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>6.398866</td>\n      <td>0.590</td>\n      <td>0.737288</td>\n      <td>0.016595</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 287)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>0.783363</td>\n      <td>0.685</td>\n      <td>0.613112</td>\n      <td>0.268817</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 221)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>0.824684</td>\n      <td>0.695</td>\n      <td>0.559283</td>\n      <td>0.310741</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 88)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>0.559807</td>\n      <td>0.510</td>\n      <td>0.507711</td>\n      <td>0.014069</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>sparse JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 68)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>0.302327</td>\n      <td>0.570</td>\n      <td>0.478668</td>\n      <td>0.042786</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 1699)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>2.212834</td>\n      <td>0.665</td>\n      <td>0.541991</td>\n      <td>0.087127</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 1699)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>2.079176</td>\n      <td>0.890</td>\n      <td>0.555577</td>\n      <td>1.606331</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 7189)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>7.440061</td>\n      <td>0.910</td>\n      <td>0.986320</td>\n      <td>1.194799</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 5526)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>6.138703</td>\n      <td>0.905</td>\n      <td>0.820940</td>\n      <td>1.196562</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 287)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>0.643513</td>\n      <td>0.780</td>\n      <td>0.585150</td>\n      <td>0.985137</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 221)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>0.599054</td>\n      <td>0.605</td>\n      <td>0.432916</td>\n      <td>0.077643</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 88)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>0.386148</td>\n      <td>0.645</td>\n      <td>0.489784</td>\n      <td>0.147281</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>JL transform</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 68)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>0.329178</td>\n      <td>0.675</td>\n      <td>0.453571</td>\n      <td>0.232054</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>PCA</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 69)</td>\n      <td>{'n_components': 69, 'svd_solver': 'auto'}</td>\n      <td>5.966820</td>\n      <td>0.900</td>\n      <td>0.496544</td>\n      <td>1.215092</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Nothing</td>\n      <td>(6726, 16990)</td>\n      <td>(6928, 16990)</td>\n      <td>{}</td>\n      <td>0.000010</td>\n      <td>0.725</td>\n      <td>1.589015</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"26\" valign=\"top\">5</th>\n      <th>0</th>\n      <td>extremely sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 4910)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>8.357938</td>\n      <td>0.638</td>\n      <td>1.393794</td>\n      <td>0.032732</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>extremely sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 4763)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>8.129092</td>\n      <td>0.374</td>\n      <td>1.204843</td>\n      <td>0.000157</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>extremely sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 2455)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>8.239602</td>\n      <td>0.524</td>\n      <td>0.820879</td>\n      <td>0.004572</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>extremely sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 2381)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>8.126419</td>\n      <td>0.954</td>\n      <td>0.790365</td>\n      <td>1.829138</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>extremely sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 491)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>8.022683</td>\n      <td>0.936</td>\n      <td>0.607731</td>\n      <td>1.511894</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>extremely sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 476)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>7.733724</td>\n      <td>0.886</td>\n      <td>0.519516</td>\n      <td>0.914573</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>extremely sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 272)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>7.717745</td>\n      <td>0.954</td>\n      <td>0.614072</td>\n      <td>1.915865</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>extremely sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 264)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>7.519420</td>\n      <td>0.902</td>\n      <td>0.641219</td>\n      <td>1.093822</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 28759)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>198.481353</td>\n      <td>0.680</td>\n      <td>4.324042</td>\n      <td>0.039497</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 22104)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>154.797251</td>\n      <td>0.604</td>\n      <td>3.471630</td>\n      <td>0.012166</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 7189)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>54.859835</td>\n      <td>0.576</td>\n      <td>1.531134</td>\n      <td>0.008040</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 5526)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>40.870175</td>\n      <td>0.742</td>\n      <td>1.055391</td>\n      <td>0.104315</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 287)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>4.095488</td>\n      <td>0.436</td>\n      <td>0.580237</td>\n      <td>0.000948</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 221)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>3.690838</td>\n      <td>0.460</td>\n      <td>0.502905</td>\n      <td>0.001832</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 88)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>2.716613</td>\n      <td>0.246</td>\n      <td>0.530785</td>\n      <td>0.000004</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>sparse JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 68)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>2.602714</td>\n      <td>0.278</td>\n      <td>0.451527</td>\n      <td>0.000014</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 28759)</td>\n      <td>{'ep': 0.05, 'de': 0.05}</td>\n      <td>188.893031</td>\n      <td>0.600</td>\n      <td>5.375067</td>\n      <td>0.011314</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 22104)</td>\n      <td>{'ep': 0.05, 'de': 0.1}</td>\n      <td>137.989284</td>\n      <td>0.590</td>\n      <td>4.317272</td>\n      <td>0.009663</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 7189)</td>\n      <td>{'ep': 0.1, 'de': 0.05}</td>\n      <td>45.495060</td>\n      <td>0.590</td>\n      <td>1.593343</td>\n      <td>0.010404</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 5526)</td>\n      <td>{'ep': 0.1, 'de': 0.1}</td>\n      <td>34.895092</td>\n      <td>0.624</td>\n      <td>1.322746</td>\n      <td>0.018831</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 287)</td>\n      <td>{'ep': 0.5, 'de': 0.05}</td>\n      <td>3.346592</td>\n      <td>0.456</td>\n      <td>0.634654</td>\n      <td>0.001679</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 221)</td>\n      <td>{'ep': 0.5, 'de': 0.1}</td>\n      <td>3.132158</td>\n      <td>0.358</td>\n      <td>0.581430</td>\n      <td>0.000149</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 88)</td>\n      <td>{'ep': 0.9, 'de': 0.05}</td>\n      <td>2.641303</td>\n      <td>0.318</td>\n      <td>0.536496</td>\n      <td>0.000054</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>JL transform</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 68)</td>\n      <td>{'ep': 0.9, 'de': 0.1}</td>\n      <td>2.438814</td>\n      <td>0.306</td>\n      <td>0.541322</td>\n      <td>0.000037</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>PCA</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 236)</td>\n      <td>{'n_components': 236, 'svd_solver': 'auto'}</td>\n      <td>60.950643</td>\n      <td>0.690</td>\n      <td>0.578181</td>\n      <td>0.048486</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Nothing</td>\n      <td>(23183, 40537)</td>\n      <td>(23688, 40537)</td>\n      <td>{}</td>\n      <td>0.000038</td>\n      <td>0.622</td>\n      <td>7.140457</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = out_df.groupby(by=\"filename\", group_keys=True).apply(lambda x: x[:])\n",
    "all[[\"name\", \"original_shape\", \"transformed_shape\", \"params\", \"reduction_time\", \"accuracy\", \"train_time\", \"score_series\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-05T07:28:09.511998Z",
     "start_time": "2023-07-05T07:28:09.496228Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
